{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T17:54:05.729026Z",
     "iopub.status.busy": "2025-10-13T17:54:05.728488Z",
     "iopub.status.idle": "2025-10-13T18:00:52.083410Z",
     "shell.execute_reply": "2025-10-13T18:00:52.082711Z",
     "shell.execute_reply.started": "2025-10-13T17:54:05.729000Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75000, 2048)\n",
      "   sample_id                                    catalog_content  \\\n",
      "0      33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n",
      "1     198967  Item Name: Salerno Cookies, The Original Butte...   \n",
      "2     261251  Item Name: Bear Creek Hearty Soup Bowl, Creamy...   \n",
      "3      55858  Item Name: Judee’s Blue Cheese Powder 11.25 oz...   \n",
      "4     292686  Item Name: kedem Sherry Cooking Wine, 12.7 Oun...   \n",
      "\n",
      "                                          image_link  price  \n",
      "0  https://m.media-amazon.com/images/I/51mo8htwTH...   4.89  \n",
      "1  https://m.media-amazon.com/images/I/71YtriIHAA...  13.12  \n",
      "2  https://m.media-amazon.com/images/I/51+PFEe-w-...   1.97  \n",
      "3  https://m.media-amazon.com/images/I/41mu0HAToD...  30.34  \n",
      "4  https://m.media-amazon.com/images/I/41sA037+Qv...  66.49  \n",
      "Running PCA reduction...\n",
      "Reduced shape: (75000, 768)\n",
      "Using device: cuda\n",
      "\n",
      "=== Training Model (Seed 42) ===\n",
      "[Seed 42] Epoch [1/400] Train Loss: 0.002457 | Val Loss: 0.228029\n",
      "[Seed 42] Epoch [2/400] Train Loss: 0.001838 | Val Loss: 0.217334\n",
      "[Seed 42] Epoch [3/400] Train Loss: 0.001678 | Val Loss: 0.207181\n",
      "[Seed 42] Epoch [4/400] Train Loss: 0.001564 | Val Loss: 0.199892\n",
      "[Seed 42] Epoch [5/400] Train Loss: 0.001463 | Val Loss: 0.196230\n",
      "[Seed 42] Epoch [6/400] Train Loss: 0.001378 | Val Loss: 0.194438\n",
      "[Seed 42] Epoch [7/400] Train Loss: 0.001298 | Val Loss: 0.193265\n",
      "[Seed 42] Epoch [8/400] Train Loss: 0.001238 | Val Loss: 0.189902\n",
      "[Seed 42] Epoch [9/400] Train Loss: 0.001184 | Val Loss: 0.190275\n",
      "[Seed 42] Epoch [10/400] Train Loss: 0.001124 | Val Loss: 0.189283\n",
      "[Seed 42] Epoch [11/400] Train Loss: 0.001068 | Val Loss: 0.190974\n",
      "[Seed 42] Epoch [12/400] Train Loss: 0.001027 | Val Loss: 0.189458\n",
      "[Seed 42] Epoch [13/400] Train Loss: 0.000993 | Val Loss: 0.189762\n",
      "[Seed 42] Epoch [14/400] Train Loss: 0.000956 | Val Loss: 0.189326\n",
      "[Seed 42] Epoch [15/400] Train Loss: 0.000923 | Val Loss: 0.190625\n",
      "[Seed 42] Epoch [16/400] Train Loss: 0.000891 | Val Loss: 0.189515\n",
      "[Seed 42] Epoch [17/400] Train Loss: 0.000863 | Val Loss: 0.190362\n",
      "[Seed 42] Epoch [18/400] Train Loss: 0.000833 | Val Loss: 0.191854\n",
      "[Seed 42] Epoch [19/400] Train Loss: 0.000802 | Val Loss: 0.189953\n",
      "[Seed 42] Epoch [20/400] Train Loss: 0.000781 | Val Loss: 0.189179\n",
      "[Seed 42] Epoch [21/400] Train Loss: 0.000763 | Val Loss: 0.189234\n",
      "[Seed 42] Epoch [22/400] Train Loss: 0.000746 | Val Loss: 0.189079\n",
      "[Seed 42] Epoch [23/400] Train Loss: 0.000725 | Val Loss: 0.190413\n",
      "[Seed 42] Epoch [24/400] Train Loss: 0.000704 | Val Loss: 0.191505\n",
      "[Seed 42] Epoch [25/400] Train Loss: 0.000688 | Val Loss: 0.191523\n",
      "[Seed 42] Epoch [26/400] Train Loss: 0.000676 | Val Loss: 0.189826\n",
      "[Seed 42] Epoch [27/400] Train Loss: 0.000655 | Val Loss: 0.190942\n",
      "[Seed 42] Epoch [28/400] Train Loss: 0.000645 | Val Loss: 0.188962\n",
      "[Seed 42] Epoch [29/400] Train Loss: 0.000637 | Val Loss: 0.189576\n",
      "[Seed 42] Epoch [30/400] Train Loss: 0.000616 | Val Loss: 0.190769\n",
      "[Seed 42] Epoch [31/400] Train Loss: 0.000615 | Val Loss: 0.190356\n",
      "[Seed 42] Epoch [32/400] Train Loss: 0.000589 | Val Loss: 0.190930\n",
      "[Seed 42] Epoch [33/400] Train Loss: 0.000588 | Val Loss: 0.190390\n",
      "[Seed 42] Epoch [34/400] Train Loss: 0.000580 | Val Loss: 0.190714\n",
      "[Seed 42] Epoch [35/400] Train Loss: 0.000573 | Val Loss: 0.190476\n",
      "[Seed 42] Epoch [36/400] Train Loss: 0.000563 | Val Loss: 0.189948\n",
      "[Seed 42] Epoch [37/400] Train Loss: 0.000557 | Val Loss: 0.190935\n",
      "[Seed 42] Epoch [38/400] Train Loss: 0.000552 | Val Loss: 0.190072\n",
      "[Seed 42] Epoch [39/400] Train Loss: 0.000544 | Val Loss: 0.191056\n",
      "[Seed 42] Epoch [40/400] Train Loss: 0.000542 | Val Loss: 0.191172\n",
      "[Seed 42] Epoch [41/400] Train Loss: 0.000535 | Val Loss: 0.190780\n",
      "[Seed 42] Epoch [42/400] Train Loss: 0.000530 | Val Loss: 0.190590\n",
      "[Seed 42] Epoch [43/400] Train Loss: 0.000527 | Val Loss: 0.190731\n",
      "[Seed 42] Epoch [44/400] Train Loss: 0.000523 | Val Loss: 0.190517\n",
      "[Seed 42] Epoch [45/400] Train Loss: 0.000526 | Val Loss: 0.190610\n",
      "[Seed 42] Epoch [46/400] Train Loss: 0.000527 | Val Loss: 0.190397\n",
      "[Seed 42] Epoch [47/400] Train Loss: 0.000523 | Val Loss: 0.190567\n",
      "[Seed 42] Epoch [48/400] Train Loss: 0.000517 | Val Loss: 0.190518\n",
      "[Seed 42] Epoch [49/400] Train Loss: 0.000525 | Val Loss: 0.190501\n",
      "[Seed 42] Epoch [50/400] Train Loss: 0.000520 | Val Loss: 0.190489\n",
      "[Seed 42] Epoch [51/400] Train Loss: 0.000516 | Val Loss: 0.190489\n",
      "[Seed 42] Epoch [52/400] Train Loss: 0.000521 | Val Loss: 0.190467\n",
      "[Seed 42] Epoch [53/400] Train Loss: 0.000523 | Val Loss: 0.190431\n",
      "[Seed 42] Epoch [54/400] Train Loss: 0.000525 | Val Loss: 0.190493\n",
      "[Seed 42] Epoch [55/400] Train Loss: 0.000524 | Val Loss: 0.190608\n",
      "[Seed 42] Epoch [56/400] Train Loss: 0.000521 | Val Loss: 0.190672\n",
      "[Seed 42] Epoch [57/400] Train Loss: 0.000524 | Val Loss: 0.190589\n",
      "[Seed 42] Epoch [58/400] Train Loss: 0.000521 | Val Loss: 0.190711\n",
      "[Seed 42] Epoch [59/400] Train Loss: 0.000521 | Val Loss: 0.190865\n",
      "[Seed 42] Epoch [60/400] Train Loss: 0.000520 | Val Loss: 0.190696\n",
      "[Seed 42] Epoch [61/400] Train Loss: 0.000515 | Val Loss: 0.191035\n",
      "[Seed 42] Epoch [62/400] Train Loss: 0.000525 | Val Loss: 0.190837\n",
      "[Seed 42] Epoch [63/400] Train Loss: 0.000522 | Val Loss: 0.191369\n",
      "[Seed 42] Epoch [64/400] Train Loss: 0.000518 | Val Loss: 0.190902\n",
      "[Seed 42] Epoch [65/400] Train Loss: 0.000514 | Val Loss: 0.191635\n",
      "[Seed 42] Epoch [66/400] Train Loss: 0.000520 | Val Loss: 0.191278\n",
      "[Seed 42] Epoch [67/400] Train Loss: 0.000515 | Val Loss: 0.191119\n",
      "[Seed 42] Epoch [68/400] Train Loss: 0.000516 | Val Loss: 0.192384\n",
      "[Seed 42] Early stopping at epoch 68\n",
      "\n",
      "=== Training Model (Seed 99) ===\n",
      "[Seed 99] Epoch [1/400] Train Loss: 0.002494 | Val Loss: 0.229027\n",
      "[Seed 99] Epoch [2/400] Train Loss: 0.001839 | Val Loss: 0.214786\n",
      "[Seed 99] Epoch [3/400] Train Loss: 0.001668 | Val Loss: 0.205628\n",
      "[Seed 99] Epoch [4/400] Train Loss: 0.001556 | Val Loss: 0.201276\n",
      "[Seed 99] Epoch [5/400] Train Loss: 0.001455 | Val Loss: 0.196370\n",
      "[Seed 99] Epoch [6/400] Train Loss: 0.001373 | Val Loss: 0.194171\n",
      "[Seed 99] Epoch [7/400] Train Loss: 0.001303 | Val Loss: 0.193213\n",
      "[Seed 99] Epoch [8/400] Train Loss: 0.001240 | Val Loss: 0.191481\n",
      "[Seed 99] Epoch [9/400] Train Loss: 0.001182 | Val Loss: 0.190424\n",
      "[Seed 99] Epoch [10/400] Train Loss: 0.001126 | Val Loss: 0.192476\n",
      "[Seed 99] Epoch [11/400] Train Loss: 0.001080 | Val Loss: 0.189377\n",
      "[Seed 99] Epoch [12/400] Train Loss: 0.001030 | Val Loss: 0.190973\n",
      "[Seed 99] Epoch [13/400] Train Loss: 0.000988 | Val Loss: 0.189976\n",
      "[Seed 99] Epoch [14/400] Train Loss: 0.000959 | Val Loss: 0.188548\n",
      "[Seed 99] Epoch [15/400] Train Loss: 0.000920 | Val Loss: 0.190787\n",
      "[Seed 99] Epoch [16/400] Train Loss: 0.000899 | Val Loss: 0.191818\n",
      "[Seed 99] Epoch [17/400] Train Loss: 0.000865 | Val Loss: 0.190663\n",
      "[Seed 99] Epoch [18/400] Train Loss: 0.000833 | Val Loss: 0.190450\n",
      "[Seed 99] Epoch [19/400] Train Loss: 0.000801 | Val Loss: 0.188570\n",
      "[Seed 99] Epoch [20/400] Train Loss: 0.000785 | Val Loss: 0.193465\n",
      "[Seed 99] Epoch [21/400] Train Loss: 0.000766 | Val Loss: 0.193121\n",
      "[Seed 99] Epoch [22/400] Train Loss: 0.000752 | Val Loss: 0.193162\n",
      "[Seed 99] Epoch [23/400] Train Loss: 0.000729 | Val Loss: 0.190136\n",
      "[Seed 99] Epoch [24/400] Train Loss: 0.000708 | Val Loss: 0.190888\n",
      "[Seed 99] Epoch [25/400] Train Loss: 0.000692 | Val Loss: 0.192121\n",
      "[Seed 99] Epoch [26/400] Train Loss: 0.000680 | Val Loss: 0.192443\n",
      "[Seed 99] Epoch [27/400] Train Loss: 0.000672 | Val Loss: 0.191817\n",
      "[Seed 99] Epoch [28/400] Train Loss: 0.000647 | Val Loss: 0.192228\n",
      "[Seed 99] Epoch [29/400] Train Loss: 0.000641 | Val Loss: 0.193596\n",
      "[Seed 99] Epoch [30/400] Train Loss: 0.000628 | Val Loss: 0.193705\n",
      "[Seed 99] Epoch [31/400] Train Loss: 0.000611 | Val Loss: 0.192753\n",
      "[Seed 99] Epoch [32/400] Train Loss: 0.000601 | Val Loss: 0.192503\n",
      "[Seed 99] Epoch [33/400] Train Loss: 0.000590 | Val Loss: 0.192188\n",
      "[Seed 99] Epoch [34/400] Train Loss: 0.000589 | Val Loss: 0.191475\n",
      "[Seed 99] Epoch [35/400] Train Loss: 0.000574 | Val Loss: 0.190817\n",
      "[Seed 99] Epoch [36/400] Train Loss: 0.000571 | Val Loss: 0.191888\n",
      "[Seed 99] Epoch [37/400] Train Loss: 0.000563 | Val Loss: 0.191726\n",
      "[Seed 99] Epoch [38/400] Train Loss: 0.000556 | Val Loss: 0.192196\n",
      "[Seed 99] Epoch [39/400] Train Loss: 0.000557 | Val Loss: 0.192046\n",
      "[Seed 99] Epoch [40/400] Train Loss: 0.000543 | Val Loss: 0.192445\n",
      "[Seed 99] Epoch [41/400] Train Loss: 0.000542 | Val Loss: 0.191429\n",
      "[Seed 99] Epoch [42/400] Train Loss: 0.000540 | Val Loss: 0.191364\n",
      "[Seed 99] Epoch [43/400] Train Loss: 0.000536 | Val Loss: 0.191731\n",
      "[Seed 99] Epoch [44/400] Train Loss: 0.000533 | Val Loss: 0.191435\n",
      "[Seed 99] Epoch [45/400] Train Loss: 0.000530 | Val Loss: 0.191599\n",
      "[Seed 99] Epoch [46/400] Train Loss: 0.000530 | Val Loss: 0.191462\n",
      "[Seed 99] Epoch [47/400] Train Loss: 0.000528 | Val Loss: 0.191405\n",
      "[Seed 99] Epoch [48/400] Train Loss: 0.000525 | Val Loss: 0.191558\n",
      "[Seed 99] Epoch [49/400] Train Loss: 0.000528 | Val Loss: 0.191518\n",
      "[Seed 99] Epoch [50/400] Train Loss: 0.000521 | Val Loss: 0.191520\n",
      "[Seed 99] Epoch [51/400] Train Loss: 0.000526 | Val Loss: 0.191520\n",
      "[Seed 99] Epoch [52/400] Train Loss: 0.000522 | Val Loss: 0.191518\n",
      "[Seed 99] Epoch [53/400] Train Loss: 0.000523 | Val Loss: 0.191577\n",
      "[Seed 99] Epoch [54/400] Train Loss: 0.000524 | Val Loss: 0.191518\n",
      "[Seed 99] Early stopping at epoch 54\n",
      "\n",
      "=== Training Model (Seed 2025) ===\n",
      "[Seed 2025] Epoch [1/400] Train Loss: 0.002512 | Val Loss: 0.233210\n",
      "[Seed 2025] Epoch [2/400] Train Loss: 0.001848 | Val Loss: 0.216444\n",
      "[Seed 2025] Epoch [3/400] Train Loss: 0.001685 | Val Loss: 0.206541\n",
      "[Seed 2025] Epoch [4/400] Train Loss: 0.001572 | Val Loss: 0.201812\n",
      "[Seed 2025] Epoch [5/400] Train Loss: 0.001478 | Val Loss: 0.196192\n",
      "[Seed 2025] Epoch [6/400] Train Loss: 0.001393 | Val Loss: 0.194825\n",
      "[Seed 2025] Epoch [7/400] Train Loss: 0.001319 | Val Loss: 0.191666\n",
      "[Seed 2025] Epoch [8/400] Train Loss: 0.001246 | Val Loss: 0.191549\n",
      "[Seed 2025] Epoch [9/400] Train Loss: 0.001191 | Val Loss: 0.187738\n",
      "[Seed 2025] Epoch [10/400] Train Loss: 0.001125 | Val Loss: 0.189477\n",
      "[Seed 2025] Epoch [11/400] Train Loss: 0.001084 | Val Loss: 0.189441\n",
      "[Seed 2025] Epoch [12/400] Train Loss: 0.001044 | Val Loss: 0.189575\n",
      "[Seed 2025] Epoch [13/400] Train Loss: 0.000993 | Val Loss: 0.189129\n",
      "[Seed 2025] Epoch [14/400] Train Loss: 0.000959 | Val Loss: 0.189306\n",
      "[Seed 2025] Epoch [15/400] Train Loss: 0.000916 | Val Loss: 0.188065\n",
      "[Seed 2025] Epoch [16/400] Train Loss: 0.000885 | Val Loss: 0.187397\n",
      "[Seed 2025] Epoch [17/400] Train Loss: 0.000863 | Val Loss: 0.186949\n",
      "[Seed 2025] Epoch [18/400] Train Loss: 0.000830 | Val Loss: 0.188215\n",
      "[Seed 2025] Epoch [19/400] Train Loss: 0.000809 | Val Loss: 0.188686\n",
      "[Seed 2025] Epoch [20/400] Train Loss: 0.000784 | Val Loss: 0.188581\n",
      "[Seed 2025] Epoch [21/400] Train Loss: 0.000769 | Val Loss: 0.189406\n",
      "[Seed 2025] Epoch [22/400] Train Loss: 0.000742 | Val Loss: 0.189370\n",
      "[Seed 2025] Epoch [23/400] Train Loss: 0.000729 | Val Loss: 0.190519\n",
      "[Seed 2025] Epoch [24/400] Train Loss: 0.000715 | Val Loss: 0.189001\n",
      "[Seed 2025] Epoch [25/400] Train Loss: 0.000690 | Val Loss: 0.189439\n",
      "[Seed 2025] Epoch [26/400] Train Loss: 0.000677 | Val Loss: 0.187995\n",
      "[Seed 2025] Epoch [27/400] Train Loss: 0.000663 | Val Loss: 0.189111\n",
      "[Seed 2025] Epoch [28/400] Train Loss: 0.000645 | Val Loss: 0.188473\n",
      "[Seed 2025] Epoch [29/400] Train Loss: 0.000637 | Val Loss: 0.188447\n",
      "[Seed 2025] Epoch [30/400] Train Loss: 0.000621 | Val Loss: 0.189026\n",
      "[Seed 2025] Epoch [31/400] Train Loss: 0.000615 | Val Loss: 0.188619\n",
      "[Seed 2025] Epoch [32/400] Train Loss: 0.000600 | Val Loss: 0.188954\n",
      "[Seed 2025] Epoch [33/400] Train Loss: 0.000593 | Val Loss: 0.188700\n",
      "[Seed 2025] Epoch [34/400] Train Loss: 0.000578 | Val Loss: 0.189076\n",
      "[Seed 2025] Epoch [35/400] Train Loss: 0.000574 | Val Loss: 0.189005\n",
      "[Seed 2025] Epoch [36/400] Train Loss: 0.000567 | Val Loss: 0.188492\n",
      "[Seed 2025] Epoch [37/400] Train Loss: 0.000562 | Val Loss: 0.189557\n",
      "[Seed 2025] Epoch [38/400] Train Loss: 0.000548 | Val Loss: 0.189110\n",
      "[Seed 2025] Epoch [39/400] Train Loss: 0.000548 | Val Loss: 0.189281\n",
      "[Seed 2025] Epoch [40/400] Train Loss: 0.000539 | Val Loss: 0.188988\n",
      "[Seed 2025] Epoch [41/400] Train Loss: 0.000537 | Val Loss: 0.188773\n",
      "[Seed 2025] Epoch [42/400] Train Loss: 0.000532 | Val Loss: 0.189325\n",
      "[Seed 2025] Epoch [43/400] Train Loss: 0.000529 | Val Loss: 0.189067\n",
      "[Seed 2025] Epoch [44/400] Train Loss: 0.000524 | Val Loss: 0.189104\n",
      "[Seed 2025] Epoch [45/400] Train Loss: 0.000526 | Val Loss: 0.189038\n",
      "[Seed 2025] Epoch [46/400] Train Loss: 0.000522 | Val Loss: 0.189025\n",
      "[Seed 2025] Epoch [47/400] Train Loss: 0.000522 | Val Loss: 0.188994\n",
      "[Seed 2025] Epoch [48/400] Train Loss: 0.000522 | Val Loss: 0.188872\n",
      "[Seed 2025] Epoch [49/400] Train Loss: 0.000519 | Val Loss: 0.188923\n",
      "[Seed 2025] Epoch [50/400] Train Loss: 0.000519 | Val Loss: 0.188915\n",
      "[Seed 2025] Epoch [51/400] Train Loss: 0.000524 | Val Loss: 0.188915\n",
      "[Seed 2025] Epoch [52/400] Train Loss: 0.000522 | Val Loss: 0.188933\n",
      "[Seed 2025] Epoch [53/400] Train Loss: 0.000525 | Val Loss: 0.188916\n",
      "[Seed 2025] Epoch [54/400] Train Loss: 0.000522 | Val Loss: 0.188942\n",
      "[Seed 2025] Epoch [55/400] Train Loss: 0.000518 | Val Loss: 0.188980\n",
      "[Seed 2025] Epoch [56/400] Train Loss: 0.000521 | Val Loss: 0.188813\n",
      "[Seed 2025] Epoch [57/400] Train Loss: 0.000525 | Val Loss: 0.188661\n",
      "[Seed 2025] Early stopping at epoch 57\n",
      "\n",
      "=== Training Model (Seed 123) ===\n",
      "[Seed 123] Epoch [1/400] Train Loss: 0.002443 | Val Loss: 0.231028\n",
      "[Seed 123] Epoch [2/400] Train Loss: 0.001830 | Val Loss: 0.214666\n",
      "[Seed 123] Epoch [3/400] Train Loss: 0.001657 | Val Loss: 0.206421\n",
      "[Seed 123] Epoch [4/400] Train Loss: 0.001541 | Val Loss: 0.201077\n",
      "[Seed 123] Epoch [5/400] Train Loss: 0.001448 | Val Loss: 0.202918\n",
      "[Seed 123] Epoch [6/400] Train Loss: 0.001366 | Val Loss: 0.195128\n",
      "[Seed 123] Epoch [7/400] Train Loss: 0.001285 | Val Loss: 0.193196\n",
      "[Seed 123] Epoch [8/400] Train Loss: 0.001219 | Val Loss: 0.191340\n",
      "[Seed 123] Epoch [9/400] Train Loss: 0.001164 | Val Loss: 0.190666\n",
      "[Seed 123] Epoch [10/400] Train Loss: 0.001102 | Val Loss: 0.190378\n",
      "[Seed 123] Epoch [11/400] Train Loss: 0.001061 | Val Loss: 0.190094\n",
      "[Seed 123] Epoch [12/400] Train Loss: 0.001019 | Val Loss: 0.188711\n",
      "[Seed 123] Epoch [13/400] Train Loss: 0.000978 | Val Loss: 0.188722\n",
      "[Seed 123] Epoch [14/400] Train Loss: 0.000938 | Val Loss: 0.191466\n",
      "[Seed 123] Epoch [15/400] Train Loss: 0.000908 | Val Loss: 0.189367\n",
      "[Seed 123] Epoch [16/400] Train Loss: 0.000879 | Val Loss: 0.189842\n",
      "[Seed 123] Epoch [17/400] Train Loss: 0.000850 | Val Loss: 0.189787\n",
      "[Seed 123] Epoch [18/400] Train Loss: 0.000822 | Val Loss: 0.190202\n",
      "[Seed 123] Epoch [19/400] Train Loss: 0.000795 | Val Loss: 0.191603\n",
      "[Seed 123] Epoch [20/400] Train Loss: 0.000775 | Val Loss: 0.191461\n",
      "[Seed 123] Epoch [21/400] Train Loss: 0.000753 | Val Loss: 0.189661\n",
      "[Seed 123] Epoch [22/400] Train Loss: 0.000734 | Val Loss: 0.189440\n",
      "[Seed 123] Epoch [23/400] Train Loss: 0.000715 | Val Loss: 0.188807\n",
      "[Seed 123] Epoch [24/400] Train Loss: 0.000692 | Val Loss: 0.193848\n",
      "[Seed 123] Epoch [25/400] Train Loss: 0.000682 | Val Loss: 0.190987\n",
      "[Seed 123] Epoch [26/400] Train Loss: 0.000666 | Val Loss: 0.189155\n",
      "[Seed 123] Epoch [27/400] Train Loss: 0.000653 | Val Loss: 0.190102\n",
      "[Seed 123] Epoch [28/400] Train Loss: 0.000643 | Val Loss: 0.190991\n",
      "[Seed 123] Epoch [29/400] Train Loss: 0.000621 | Val Loss: 0.190419\n",
      "[Seed 123] Epoch [30/400] Train Loss: 0.000614 | Val Loss: 0.190533\n",
      "[Seed 123] Epoch [31/400] Train Loss: 0.000603 | Val Loss: 0.189792\n",
      "[Seed 123] Epoch [32/400] Train Loss: 0.000593 | Val Loss: 0.190926\n",
      "[Seed 123] Epoch [33/400] Train Loss: 0.000576 | Val Loss: 0.190029\n",
      "[Seed 123] Epoch [34/400] Train Loss: 0.000568 | Val Loss: 0.190867\n",
      "[Seed 123] Epoch [35/400] Train Loss: 0.000560 | Val Loss: 0.190541\n",
      "[Seed 123] Epoch [36/400] Train Loss: 0.000560 | Val Loss: 0.190485\n",
      "[Seed 123] Epoch [37/400] Train Loss: 0.000553 | Val Loss: 0.190034\n",
      "[Seed 123] Epoch [38/400] Train Loss: 0.000540 | Val Loss: 0.189870\n",
      "[Seed 123] Epoch [39/400] Train Loss: 0.000545 | Val Loss: 0.189523\n",
      "[Seed 123] Epoch [40/400] Train Loss: 0.000536 | Val Loss: 0.189495\n",
      "[Seed 123] Epoch [41/400] Train Loss: 0.000531 | Val Loss: 0.190110\n",
      "[Seed 123] Epoch [42/400] Train Loss: 0.000529 | Val Loss: 0.189194\n",
      "[Seed 123] Epoch [43/400] Train Loss: 0.000520 | Val Loss: 0.190018\n",
      "[Seed 123] Epoch [44/400] Train Loss: 0.000522 | Val Loss: 0.190219\n",
      "[Seed 123] Epoch [45/400] Train Loss: 0.000520 | Val Loss: 0.190077\n",
      "[Seed 123] Epoch [46/400] Train Loss: 0.000514 | Val Loss: 0.190088\n",
      "[Seed 123] Epoch [47/400] Train Loss: 0.000518 | Val Loss: 0.189959\n",
      "[Seed 123] Epoch [48/400] Train Loss: 0.000517 | Val Loss: 0.189882\n",
      "[Seed 123] Epoch [49/400] Train Loss: 0.000516 | Val Loss: 0.189975\n",
      "[Seed 123] Epoch [50/400] Train Loss: 0.000515 | Val Loss: 0.189972\n",
      "[Seed 123] Epoch [51/400] Train Loss: 0.000520 | Val Loss: 0.189972\n",
      "[Seed 123] Epoch [52/400] Train Loss: 0.000511 | Val Loss: 0.189950\n",
      "[Seed 123] Early stopping at epoch 52\n",
      "\n",
      "=== Training Model (Seed 777) ===\n",
      "[Seed 777] Epoch [1/400] Train Loss: 0.002431 | Val Loss: 0.230961\n",
      "[Seed 777] Epoch [2/400] Train Loss: 0.001828 | Val Loss: 0.215493\n",
      "[Seed 777] Epoch [3/400] Train Loss: 0.001662 | Val Loss: 0.206944\n",
      "[Seed 777] Epoch [4/400] Train Loss: 0.001553 | Val Loss: 0.200311\n",
      "[Seed 777] Epoch [5/400] Train Loss: 0.001456 | Val Loss: 0.198442\n",
      "[Seed 777] Epoch [6/400] Train Loss: 0.001367 | Val Loss: 0.195275\n",
      "[Seed 777] Epoch [7/400] Train Loss: 0.001300 | Val Loss: 0.193291\n",
      "[Seed 777] Epoch [8/400] Train Loss: 0.001233 | Val Loss: 0.190558\n",
      "[Seed 777] Epoch [9/400] Train Loss: 0.001175 | Val Loss: 0.190351\n",
      "[Seed 777] Epoch [10/400] Train Loss: 0.001121 | Val Loss: 0.189981\n",
      "[Seed 777] Epoch [11/400] Train Loss: 0.001074 | Val Loss: 0.189745\n",
      "[Seed 777] Epoch [12/400] Train Loss: 0.001025 | Val Loss: 0.189225\n",
      "[Seed 777] Epoch [13/400] Train Loss: 0.000983 | Val Loss: 0.189152\n",
      "[Seed 777] Epoch [14/400] Train Loss: 0.000951 | Val Loss: 0.190035\n",
      "[Seed 777] Epoch [15/400] Train Loss: 0.000919 | Val Loss: 0.188434\n",
      "[Seed 777] Epoch [16/400] Train Loss: 0.000881 | Val Loss: 0.189650\n",
      "[Seed 777] Epoch [17/400] Train Loss: 0.000859 | Val Loss: 0.190120\n",
      "[Seed 777] Epoch [18/400] Train Loss: 0.000838 | Val Loss: 0.190308\n",
      "[Seed 777] Epoch [19/400] Train Loss: 0.000812 | Val Loss: 0.191950\n",
      "[Seed 777] Epoch [20/400] Train Loss: 0.000783 | Val Loss: 0.191760\n",
      "[Seed 777] Epoch [21/400] Train Loss: 0.000757 | Val Loss: 0.190692\n",
      "[Seed 777] Epoch [22/400] Train Loss: 0.000740 | Val Loss: 0.190501\n",
      "[Seed 777] Epoch [23/400] Train Loss: 0.000723 | Val Loss: 0.192853\n",
      "[Seed 777] Epoch [24/400] Train Loss: 0.000704 | Val Loss: 0.191885\n",
      "[Seed 777] Epoch [25/400] Train Loss: 0.000687 | Val Loss: 0.191841\n",
      "[Seed 777] Epoch [26/400] Train Loss: 0.000679 | Val Loss: 0.191328\n",
      "[Seed 777] Epoch [27/400] Train Loss: 0.000663 | Val Loss: 0.192369\n",
      "[Seed 777] Epoch [28/400] Train Loss: 0.000642 | Val Loss: 0.192168\n",
      "[Seed 777] Epoch [29/400] Train Loss: 0.000631 | Val Loss: 0.192406\n",
      "[Seed 777] Epoch [30/400] Train Loss: 0.000622 | Val Loss: 0.193318\n",
      "[Seed 777] Epoch [31/400] Train Loss: 0.000608 | Val Loss: 0.193218\n",
      "[Seed 777] Epoch [32/400] Train Loss: 0.000601 | Val Loss: 0.192481\n",
      "[Seed 777] Epoch [33/400] Train Loss: 0.000587 | Val Loss: 0.190817\n",
      "[Seed 777] Epoch [34/400] Train Loss: 0.000577 | Val Loss: 0.190466\n",
      "[Seed 777] Epoch [35/400] Train Loss: 0.000565 | Val Loss: 0.192303\n",
      "[Seed 777] Epoch [36/400] Train Loss: 0.000557 | Val Loss: 0.191974\n",
      "[Seed 777] Epoch [37/400] Train Loss: 0.000552 | Val Loss: 0.191042\n",
      "[Seed 777] Epoch [38/400] Train Loss: 0.000552 | Val Loss: 0.191180\n",
      "[Seed 777] Epoch [39/400] Train Loss: 0.000546 | Val Loss: 0.191898\n",
      "[Seed 777] Epoch [40/400] Train Loss: 0.000542 | Val Loss: 0.191768\n",
      "[Seed 777] Epoch [41/400] Train Loss: 0.000536 | Val Loss: 0.192134\n",
      "[Seed 777] Epoch [42/400] Train Loss: 0.000530 | Val Loss: 0.191532\n",
      "[Seed 777] Epoch [43/400] Train Loss: 0.000531 | Val Loss: 0.191504\n",
      "[Seed 777] Epoch [44/400] Train Loss: 0.000526 | Val Loss: 0.191534\n",
      "[Seed 777] Epoch [45/400] Train Loss: 0.000522 | Val Loss: 0.191472\n",
      "[Seed 777] Epoch [46/400] Train Loss: 0.000523 | Val Loss: 0.191648\n",
      "[Seed 777] Epoch [47/400] Train Loss: 0.000524 | Val Loss: 0.191673\n",
      "[Seed 777] Epoch [48/400] Train Loss: 0.000521 | Val Loss: 0.191582\n",
      "[Seed 777] Epoch [49/400] Train Loss: 0.000522 | Val Loss: 0.191615\n",
      "[Seed 777] Epoch [50/400] Train Loss: 0.000520 | Val Loss: 0.191606\n",
      "[Seed 777] Epoch [51/400] Train Loss: 0.000521 | Val Loss: 0.191606\n",
      "[Seed 777] Epoch [52/400] Train Loss: 0.000515 | Val Loss: 0.191588\n",
      "[Seed 777] Epoch [53/400] Train Loss: 0.000517 | Val Loss: 0.191669\n",
      "[Seed 777] Epoch [54/400] Train Loss: 0.000519 | Val Loss: 0.191606\n",
      "[Seed 777] Epoch [55/400] Train Loss: 0.000520 | Val Loss: 0.191633\n",
      "[Seed 777] Early stopping at epoch 55\n",
      "\n",
      "=====================\n",
      "Final Ensemble R²: 0.5299\n",
      "Final Ensemble MAE: 6.4585\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "# ======================================================\n",
    "# STEP 1 — Load & Prepare Data\n",
    "# ======================================================\n",
    "embeddings = np.load('/kaggle/input/embeddings/train_embeddings.npy')\n",
    "df = pd.read_csv('/kaggle/input/train-llm/train.csv')\n",
    "\n",
    "print(embeddings.shape)\n",
    "print(df.head())\n",
    "\n",
    "X = embeddings\n",
    "Y = df['price'].reset_index(drop=True)\n",
    "\n",
    "# Convert to numpy if tensors\n",
    "if isinstance(X, torch.Tensor):\n",
    "    X = X.detach().cpu().numpy()\n",
    "if isinstance(Y, torch.Tensor):\n",
    "    Y = Y.detach().cpu().numpy()\n",
    "\n",
    "# Optional: clip extreme values to reduce SMAPE impact\n",
    "Y_clipped = np.clip(Y, 0, 100)  # adjust upper limit if needed\n",
    "Y_log = np.log1p(Y_clipped)\n",
    "\n",
    "# Standardize embeddings\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Dimensionality Reduction (768 components)\n",
    "print(\"Running PCA reduction...\")\n",
    "pca = PCA(n_components=768, random_state=42)\n",
    "X_reduced = pca.fit_transform(X_scaled)\n",
    "print(\"Reduced shape:\", X_reduced.shape)\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_reduced, Y_log, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert Series to numpy\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ======================================================\n",
    "# STEP 2 — Define Model (with slightly higher dropout)\n",
    "# ======================================================\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, dropout=0.35):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class OptimizedMLP(nn.Module):\n",
    "    def __init__(self, input_dim, dropout=0.35):\n",
    "        super().__init__()\n",
    "        self.fc_in = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.res1 = ResidualBlock(256, dropout)\n",
    "        self.fc_mid = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.res2 = ResidualBlock(128, dropout)\n",
    "        self.fc_out = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_in(x)\n",
    "        x = self.res1(x)\n",
    "        x = self.fc_mid(x)\n",
    "        x = self.res2(x)\n",
    "        return self.fc_out(x)\n",
    "\n",
    "# ======================================================\n",
    "# STEP 3 — Training Function (Reusable)\n",
    "# ======================================================\n",
    "def train_single_model(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = OptimizedMLP(input_dim, dropout=0.35).to(device)\n",
    "\n",
    "    criterion = nn.HuberLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-3)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "\n",
    "    epochs = 400\n",
    "    batch_size = 128\n",
    "    early_stop_patience = 40\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        permutation = torch.randperm(X_train.size(0))\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for i in range(0, X_train.size(0), batch_size):\n",
    "            idx = permutation[i:i + batch_size]\n",
    "            batch_x = X_train[idx].to(device)\n",
    "            batch_y = y_train[idx].to(device)\n",
    "\n",
    "            # Slightly increased Gaussian noise\n",
    "            batch_x = batch_x + 0.015 * torch.randn_like(batch_x)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_preds = model(X_val.to(device))\n",
    "            val_loss = criterion(val_preds, y_val.to(device))\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f\"[Seed {seed}] Epoch [{epoch+1}/{epochs}] \"\n",
    "              f\"Train Loss: {total_loss/len(X_train):.6f} | Val Loss: {val_loss.item():.6f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss.item() < best_val_loss - 1e-5:\n",
    "            best_val_loss = val_loss.item()\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stop_patience:\n",
    "                print(f\"[Seed {seed}] Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "# ======================================================\n",
    "# STEP 4 — Train 5 Models (Ensemble)\n",
    "# ======================================================\n",
    "models = []\n",
    "seeds = [42, 99, 2025, 123, 777]\n",
    "\n",
    "for s in seeds:\n",
    "    print(f\"\\n=== Training Model (Seed {s}) ===\")\n",
    "    model_s = train_single_model(s)\n",
    "    models.append(model_s)\n",
    "\n",
    "# ======================================================\n",
    "# STEP 5 — Inference (Averaged Ensemble)\n",
    "# ======================================================\n",
    "X_full = torch.tensor(X_reduced, dtype=torch.float32).to(device)\n",
    "\n",
    "ensemble_preds = []\n",
    "for model in models:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_log = model(X_full).cpu().numpy()\n",
    "        ensemble_preds.append(np.expm1(pred_log))  # inverse log-transform\n",
    "\n",
    "# Average predictions\n",
    "y_pred = np.mean(ensemble_preds, axis=0)\n",
    "y_pred = np.maximum(y_pred, 0)\n",
    "\n",
    "# Evaluation on training subset\n",
    "r2 = r2_score(Y, y_pred[:len(Y)])\n",
    "mae = mean_absolute_error(Y, y_pred[:len(Y)])\n",
    "\n",
    "print(\"\\n=====================\")\n",
    "print(f\"Final Ensemble R²: {r2:.4f}\")\n",
    "print(f\"Final Ensemble MAE: {mae:.4f}\")\n",
    "print(\"=====================\")\n",
    "\n",
    "# # Save predictions\n",
    "# np.save(\"ensemble_5models_predictions.npy\", y_pred)\n",
    "# pd.DataFrame({\"price\": y_pred.flatten()}).to_csv(\"submission_5models.csv\", index=False)\n",
    "# print(\"Saved predictions → ensemble_5models_predictions.npy & submission_5models.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T18:01:28.945159Z",
     "iopub.status.busy": "2025-10-13T18:01:28.944357Z",
     "iopub.status.idle": "2025-10-13T18:01:31.802223Z",
     "shell.execute_reply": "2025-10-13T18:01:31.801484Z",
     "shell.execute_reply.started": "2025-10-13T18:01:28.945110Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded test embeddings: (75000, 2048)\n",
      "Reduced test shape: (75000, 768)\n",
      "Model 1 done.\n",
      "Model 2 done.\n",
      "Model 3 done.\n",
      "Model 4 done.\n",
      "Model 5 done.\n",
      "Final predictions shape: (75000, 1)\n",
      "Saved → test_ensemble_5models.npy & submission_5models.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ======================================================\n",
    "# STEP 1 — Load Test Embeddings\n",
    "# ======================================================\n",
    "test_embeddings = np.load(\"/kaggle/input/embeddings/full_embeddings.npy\")\n",
    "print(\"Loaded test embeddings:\", test_embeddings.shape)\n",
    "\n",
    "# ======================================================\n",
    "# STEP 2 — Apply SAME Scaler & PCA from training\n",
    "# ======================================================\n",
    "# ⚠️ Make sure 'scaler' and 'pca' objects are from training\n",
    "X_test_scaled = scaler.transform(test_embeddings)\n",
    "X_test_reduced = pca.transform(X_test_scaled)\n",
    "print(\"Reduced test shape:\", X_test_reduced.shape)\n",
    "\n",
    "# Convert to tensor\n",
    "X_test_tensor = torch.tensor(X_test_reduced, dtype=torch.float32).to(device)\n",
    "\n",
    "# ======================================================\n",
    "# STEP 3 — Ensemble Predictions (5 models)\n",
    "# ======================================================\n",
    "ensemble_preds = []\n",
    "\n",
    "for idx, model in enumerate(models):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_log = model(X_test_tensor).cpu().numpy()\n",
    "        pred = np.expm1(pred_log)  # inverse log1p\n",
    "        ensemble_preds.append(pred)\n",
    "        print(f\"Model {idx+1} done.\")\n",
    "\n",
    "# Average predictions\n",
    "y_test_pred = np.mean(ensemble_preds, axis=0)\n",
    "y_test_pred = np.maximum(y_test_pred, 0)  # avoid negatives\n",
    "\n",
    "print(\"Final predictions shape:\", y_test_pred.shape)\n",
    "\n",
    "# ======================================================\n",
    "# STEP 4 — Save Predictions\n",
    "# ======================================================\n",
    "# np.save(\"test_ensemble_5models.npy\", y_test_pred)\n",
    "pd.DataFrame({\"price\": y_test_pred.flatten()}).to_csv(\"test_out1.csv\", index=False)\n",
    "print(\"Saved → test_ensemble_5models.npy & submission_5models.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_pred = pd.DataFrame(y_pred, columns=['Predicted_Price'])\n",
    "print(df_pred.shape)  # Should print (75000, 1)\n",
    "print(df_pred.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([df1['sample_id'], df_pred['price']],axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(\"test_out.csv\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8484308,
     "sourceId": 13373111,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8476825,
     "sourceId": 13363517,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
