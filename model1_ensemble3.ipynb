{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13373111,"sourceType":"datasetVersion","datasetId":8484308},{"sourceId":13363517,"sourceType":"datasetVersion","datasetId":8476825}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_absolute_error\n\n# ======================================================\n# STEP 1 — Load & Prepare Data\n# ======================================================\nembeddings = np.load('/kaggle/input/embeddings/train_embeddings.npy')\ndf = pd.read_csv('/kaggle/input/train-llm/train.csv')\n\nprint(embeddings.shape)\nprint(df.head())\n\nX = embeddings\nY = df['price'].reset_index(drop=True)\n\n# Convert to numpy if tensors\nif isinstance(X, torch.Tensor):\n    X = X.detach().cpu().numpy()\nif isinstance(Y, torch.Tensor):\n    Y = Y.detach().cpu().numpy()\n\n# Log-transform target\nY_log = np.log1p(Y)\n\n# Standardize embeddings\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Dimensionality Reduction\nprint(\"Running PCA reduction...\")\npca = PCA(n_components=512, random_state=42)\nX_reduced = pca.fit_transform(X_scaled)\nprint(\"Reduced shape:\", X_reduced.shape)\n\n# Split data\nX_train, X_val, y_train, y_val = train_test_split(\n    X_reduced, Y_log, test_size=0.2, random_state=42\n)\n\n# ✅ Fix ValueError: convert Series to NumPy\ny_train = np.array(y_train)\ny_val = np.array(y_val)\n\n# Convert to torch tensors\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\nX_val = torch.tensor(X_val, dtype=torch.float32)\ny_val = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\n# ======================================================\n# STEP 2 — Define Model (same as before)\n# ======================================================\nclass ResidualBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.Linear(dim, dim),\n            nn.LayerNorm(dim),\n            nn.GELU(),\n            nn.Dropout(0.3)\n        )\n    def forward(self, x):\n        return x + self.block(x)\n\nclass OptimizedMLP(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.fc_in = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.LayerNorm(256),\n            nn.GELU(),\n            nn.Dropout(0.3)\n        )\n        self.res1 = ResidualBlock(256)\n        self.fc_mid = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.LayerNorm(128),\n            nn.GELU(),\n            nn.Dropout(0.3)\n        )\n        self.res2 = ResidualBlock(128)\n        self.fc_out = nn.Linear(128, 1)\n\n    def forward(self, x):\n        x = self.fc_in(x)\n        x = self.res1(x)\n        x = self.fc_mid(x)\n        x = self.res2(x)\n        return self.fc_out(x)\n\n\n# ======================================================\n# STEP 3 — Training Function (Reusable)\n# ======================================================\ndef train_single_model(seed):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n\n    input_dim = X_train.shape[1]\n    model = OptimizedMLP(input_dim).to(device)\n\n    criterion = nn.HuberLoss()  # more robust than MSE\n    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-3)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n\n    epochs = 400\n    batch_size = 128\n    early_stop_patience = 40\n    best_val_loss = float('inf')\n    patience_counter = 0\n\n    for epoch in range(epochs):\n        model.train()\n        permutation = torch.randperm(X_train.size(0))\n        total_loss = 0.0\n\n        for i in range(0, X_train.size(0), batch_size):\n            idx = permutation[i:i + batch_size]\n            batch_x = X_train[idx].to(device)\n            batch_y = y_train[idx].to(device)\n\n            # small Gaussian noise for regularization\n            batch_x = batch_x + 0.01 * torch.randn_like(batch_x)\n\n            optimizer.zero_grad()\n            outputs = model(batch_x)\n            loss = criterion(outputs, batch_y)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n        # Validation\n        model.eval()\n        with torch.no_grad():\n            val_preds = model(X_val.to(device))\n            val_loss = criterion(val_preds, y_val.to(device))\n\n        scheduler.step()\n\n        print(f\"[Seed {seed}] Epoch [{epoch+1}/{epochs}] \"\n              f\"Train Loss: {total_loss/len(X_train):.6f} | Val Loss: {val_loss.item():.6f}\")\n\n        # Early stopping\n        if val_loss.item() < best_val_loss - 1e-5:\n            best_val_loss = val_loss.item()\n            patience_counter = 0\n            best_model_state = model.state_dict()\n        else:\n            patience_counter += 1\n            if patience_counter >= early_stop_patience:\n                print(f\"[Seed {seed}] Early stopping at epoch {epoch+1}\")\n                break\n\n    model.load_state_dict(best_model_state)\n    return model\n\n\n# ======================================================\n# STEP 4 — Train 3 Models (Ensemble)\n# ======================================================\nmodels = []\nseeds = [42, 99, 2025]\n\nfor s in seeds:\n    print(f\"\\n=== Training Model (Seed {s}) ===\")\n    model_s = train_single_model(s)\n    models.append(model_s)\n\n\n# ======================================================\n# STEP 5 — Inference (Averaged Ensemble)\n# ======================================================\nX_full = torch.tensor(X_reduced, dtype=torch.float32).to(device)\n\nensemble_preds = []\nfor model in models:\n    model.eval()\n    with torch.no_grad():\n        pred_log = model(X_full).cpu().numpy()\n        ensemble_preds.append(np.expm1(pred_log))  # inverse log-transform\n\n# Average predictions\ny_pred = np.mean(ensemble_preds, axis=0)\ny_pred = np.maximum(y_pred, 0)\n\n# Evaluation on training subset\nr2 = r2_score(Y, y_pred[:len(Y)])\nmae = mean_absolute_error(Y, y_pred[:len(Y)])\n\nprint(\"\\n=====================\")\nprint(f\"Final Ensemble R²: {r2:.4f}\")\nprint(f\"Final Ensemble MAE: {mae:.4f}\")\nprint(\"=====================\")\n\n# # Optional: Save ensemble predictions\n# np.save(\"ensemble_predictions.npy\", y_pred_ensemble)\n# print(\"Saved predictions → ensemble_predictions.npy\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:23:16.087789Z","iopub.execute_input":"2025-10-13T17:23:16.088017Z","iopub.status.idle":"2025-10-13T17:27:25.054506Z","shell.execute_reply.started":"2025-10-13T17:23:16.087999Z","shell.execute_reply":"2025-10-13T17:27:25.053797Z"}},"outputs":[{"name":"stdout","text":"(75000, 2048)\n   sample_id                                    catalog_content  \\\n0      33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n1     198967  Item Name: Salerno Cookies, The Original Butte...   \n2     261251  Item Name: Bear Creek Hearty Soup Bowl, Creamy...   \n3      55858  Item Name: Judee’s Blue Cheese Powder 11.25 oz...   \n4     292686  Item Name: kedem Sherry Cooking Wine, 12.7 Oun...   \n\n                                          image_link  price  \n0  https://m.media-amazon.com/images/I/51mo8htwTH...   4.89  \n1  https://m.media-amazon.com/images/I/71YtriIHAA...  13.12  \n2  https://m.media-amazon.com/images/I/51+PFEe-w-...   1.97  \n3  https://m.media-amazon.com/images/I/41mu0HAToD...  30.34  \n4  https://m.media-amazon.com/images/I/41sA037+Qv...  66.49  \nRunning PCA reduction...\nReduced shape: (75000, 512)\nUsing device: cuda\n\n=== Training Model (Seed 42) ===\n[Seed 42] Epoch [1/400] Train Loss: 0.002526 | Val Loss: 0.244138\n[Seed 42] Epoch [2/400] Train Loss: 0.001893 | Val Loss: 0.229102\n[Seed 42] Epoch [3/400] Train Loss: 0.001734 | Val Loss: 0.219628\n[Seed 42] Epoch [4/400] Train Loss: 0.001612 | Val Loss: 0.211365\n[Seed 42] Epoch [5/400] Train Loss: 0.001522 | Val Loss: 0.209238\n[Seed 42] Epoch [6/400] Train Loss: 0.001438 | Val Loss: 0.204746\n[Seed 42] Epoch [7/400] Train Loss: 0.001366 | Val Loss: 0.202417\n[Seed 42] Epoch [8/400] Train Loss: 0.001299 | Val Loss: 0.199673\n[Seed 42] Epoch [9/400] Train Loss: 0.001245 | Val Loss: 0.200561\n[Seed 42] Epoch [10/400] Train Loss: 0.001198 | Val Loss: 0.200367\n[Seed 42] Epoch [11/400] Train Loss: 0.001159 | Val Loss: 0.198072\n[Seed 42] Epoch [12/400] Train Loss: 0.001108 | Val Loss: 0.201039\n[Seed 42] Epoch [13/400] Train Loss: 0.001071 | Val Loss: 0.199538\n[Seed 42] Epoch [14/400] Train Loss: 0.001039 | Val Loss: 0.199775\n[Seed 42] Epoch [15/400] Train Loss: 0.001011 | Val Loss: 0.198079\n[Seed 42] Epoch [16/400] Train Loss: 0.000980 | Val Loss: 0.197707\n[Seed 42] Epoch [17/400] Train Loss: 0.000950 | Val Loss: 0.199481\n[Seed 42] Epoch [18/400] Train Loss: 0.000934 | Val Loss: 0.197775\n[Seed 42] Epoch [19/400] Train Loss: 0.000909 | Val Loss: 0.199128\n[Seed 42] Epoch [20/400] Train Loss: 0.000882 | Val Loss: 0.199113\n[Seed 42] Epoch [21/400] Train Loss: 0.000864 | Val Loss: 0.199040\n[Seed 42] Epoch [22/400] Train Loss: 0.000847 | Val Loss: 0.198713\n[Seed 42] Epoch [23/400] Train Loss: 0.000825 | Val Loss: 0.198913\n[Seed 42] Epoch [24/400] Train Loss: 0.000808 | Val Loss: 0.200773\n[Seed 42] Epoch [25/400] Train Loss: 0.000788 | Val Loss: 0.200106\n[Seed 42] Epoch [26/400] Train Loss: 0.000784 | Val Loss: 0.200519\n[Seed 42] Epoch [27/400] Train Loss: 0.000771 | Val Loss: 0.198998\n[Seed 42] Epoch [28/400] Train Loss: 0.000753 | Val Loss: 0.198477\n[Seed 42] Epoch [29/400] Train Loss: 0.000752 | Val Loss: 0.200283\n[Seed 42] Epoch [30/400] Train Loss: 0.000728 | Val Loss: 0.200152\n[Seed 42] Epoch [31/400] Train Loss: 0.000722 | Val Loss: 0.200638\n[Seed 42] Epoch [32/400] Train Loss: 0.000712 | Val Loss: 0.200475\n[Seed 42] Epoch [33/400] Train Loss: 0.000698 | Val Loss: 0.199510\n[Seed 42] Epoch [34/400] Train Loss: 0.000695 | Val Loss: 0.199903\n[Seed 42] Epoch [35/400] Train Loss: 0.000678 | Val Loss: 0.201080\n[Seed 42] Epoch [36/400] Train Loss: 0.000676 | Val Loss: 0.199340\n[Seed 42] Epoch [37/400] Train Loss: 0.000663 | Val Loss: 0.199510\n[Seed 42] Epoch [38/400] Train Loss: 0.000657 | Val Loss: 0.200388\n[Seed 42] Epoch [39/400] Train Loss: 0.000656 | Val Loss: 0.199935\n[Seed 42] Epoch [40/400] Train Loss: 0.000653 | Val Loss: 0.200373\n[Seed 42] Epoch [41/400] Train Loss: 0.000646 | Val Loss: 0.198991\n[Seed 42] Epoch [42/400] Train Loss: 0.000639 | Val Loss: 0.199903\n[Seed 42] Epoch [43/400] Train Loss: 0.000640 | Val Loss: 0.199573\n[Seed 42] Epoch [44/400] Train Loss: 0.000640 | Val Loss: 0.199446\n[Seed 42] Epoch [45/400] Train Loss: 0.000639 | Val Loss: 0.199493\n[Seed 42] Epoch [46/400] Train Loss: 0.000635 | Val Loss: 0.199736\n[Seed 42] Epoch [47/400] Train Loss: 0.000635 | Val Loss: 0.199796\n[Seed 42] Epoch [48/400] Train Loss: 0.000628 | Val Loss: 0.199762\n[Seed 42] Epoch [49/400] Train Loss: 0.000632 | Val Loss: 0.199877\n[Seed 42] Epoch [50/400] Train Loss: 0.000630 | Val Loss: 0.199857\n[Seed 42] Epoch [51/400] Train Loss: 0.000634 | Val Loss: 0.199857\n[Seed 42] Epoch [52/400] Train Loss: 0.000632 | Val Loss: 0.199829\n[Seed 42] Epoch [53/400] Train Loss: 0.000628 | Val Loss: 0.199708\n[Seed 42] Epoch [54/400] Train Loss: 0.000628 | Val Loss: 0.199773\n[Seed 42] Epoch [55/400] Train Loss: 0.000621 | Val Loss: 0.199843\n[Seed 42] Epoch [56/400] Train Loss: 0.000636 | Val Loss: 0.199695\n[Seed 42] Early stopping at epoch 56\n\n=== Training Model (Seed 99) ===\n[Seed 99] Epoch [1/400] Train Loss: 0.002545 | Val Loss: 0.244000\n[Seed 99] Epoch [2/400] Train Loss: 0.001902 | Val Loss: 0.226366\n[Seed 99] Epoch [3/400] Train Loss: 0.001741 | Val Loss: 0.219025\n[Seed 99] Epoch [4/400] Train Loss: 0.001619 | Val Loss: 0.214318\n[Seed 99] Epoch [5/400] Train Loss: 0.001530 | Val Loss: 0.209670\n[Seed 99] Epoch [6/400] Train Loss: 0.001445 | Val Loss: 0.204755\n[Seed 99] Epoch [7/400] Train Loss: 0.001379 | Val Loss: 0.201512\n[Seed 99] Epoch [8/400] Train Loss: 0.001313 | Val Loss: 0.201857\n[Seed 99] Epoch [9/400] Train Loss: 0.001258 | Val Loss: 0.200395\n[Seed 99] Epoch [10/400] Train Loss: 0.001208 | Val Loss: 0.198917\n[Seed 99] Epoch [11/400] Train Loss: 0.001164 | Val Loss: 0.198425\n[Seed 99] Epoch [12/400] Train Loss: 0.001117 | Val Loss: 0.198913\n[Seed 99] Epoch [13/400] Train Loss: 0.001080 | Val Loss: 0.198595\n[Seed 99] Epoch [14/400] Train Loss: 0.001052 | Val Loss: 0.198434\n[Seed 99] Epoch [15/400] Train Loss: 0.001007 | Val Loss: 0.199025\n[Seed 99] Epoch [16/400] Train Loss: 0.000981 | Val Loss: 0.199093\n[Seed 99] Epoch [17/400] Train Loss: 0.000958 | Val Loss: 0.196629\n[Seed 99] Epoch [18/400] Train Loss: 0.000940 | Val Loss: 0.198790\n[Seed 99] Epoch [19/400] Train Loss: 0.000913 | Val Loss: 0.199293\n[Seed 99] Epoch [20/400] Train Loss: 0.000892 | Val Loss: 0.199170\n[Seed 99] Epoch [21/400] Train Loss: 0.000870 | Val Loss: 0.199748\n[Seed 99] Epoch [22/400] Train Loss: 0.000847 | Val Loss: 0.198801\n[Seed 99] Epoch [23/400] Train Loss: 0.000834 | Val Loss: 0.199238\n[Seed 99] Epoch [24/400] Train Loss: 0.000818 | Val Loss: 0.200958\n[Seed 99] Epoch [25/400] Train Loss: 0.000801 | Val Loss: 0.200120\n[Seed 99] Epoch [26/400] Train Loss: 0.000775 | Val Loss: 0.201127\n[Seed 99] Epoch [27/400] Train Loss: 0.000772 | Val Loss: 0.200801\n[Seed 99] Epoch [28/400] Train Loss: 0.000757 | Val Loss: 0.199929\n[Seed 99] Epoch [29/400] Train Loss: 0.000741 | Val Loss: 0.199265\n[Seed 99] Epoch [30/400] Train Loss: 0.000731 | Val Loss: 0.199710\n[Seed 99] Epoch [31/400] Train Loss: 0.000715 | Val Loss: 0.201739\n[Seed 99] Epoch [32/400] Train Loss: 0.000706 | Val Loss: 0.200264\n[Seed 99] Epoch [33/400] Train Loss: 0.000711 | Val Loss: 0.200652\n[Seed 99] Epoch [34/400] Train Loss: 0.000692 | Val Loss: 0.200271\n[Seed 99] Epoch [35/400] Train Loss: 0.000683 | Val Loss: 0.200569\n[Seed 99] Epoch [36/400] Train Loss: 0.000674 | Val Loss: 0.200406\n[Seed 99] Epoch [37/400] Train Loss: 0.000670 | Val Loss: 0.200483\n[Seed 99] Epoch [38/400] Train Loss: 0.000665 | Val Loss: 0.200861\n[Seed 99] Epoch [39/400] Train Loss: 0.000655 | Val Loss: 0.200279\n[Seed 99] Epoch [40/400] Train Loss: 0.000646 | Val Loss: 0.201114\n[Seed 99] Epoch [41/400] Train Loss: 0.000647 | Val Loss: 0.200488\n[Seed 99] Epoch [42/400] Train Loss: 0.000642 | Val Loss: 0.200732\n[Seed 99] Epoch [43/400] Train Loss: 0.000643 | Val Loss: 0.200557\n[Seed 99] Epoch [44/400] Train Loss: 0.000637 | Val Loss: 0.201583\n[Seed 99] Epoch [45/400] Train Loss: 0.000642 | Val Loss: 0.200735\n[Seed 99] Epoch [46/400] Train Loss: 0.000633 | Val Loss: 0.201033\n[Seed 99] Epoch [47/400] Train Loss: 0.000637 | Val Loss: 0.200696\n[Seed 99] Epoch [48/400] Train Loss: 0.000637 | Val Loss: 0.200653\n[Seed 99] Epoch [49/400] Train Loss: 0.000631 | Val Loss: 0.200689\n[Seed 99] Epoch [50/400] Train Loss: 0.000633 | Val Loss: 0.200779\n[Seed 99] Epoch [51/400] Train Loss: 0.000633 | Val Loss: 0.200779\n[Seed 99] Epoch [52/400] Train Loss: 0.000634 | Val Loss: 0.200738\n[Seed 99] Epoch [53/400] Train Loss: 0.000639 | Val Loss: 0.200737\n[Seed 99] Epoch [54/400] Train Loss: 0.000631 | Val Loss: 0.200789\n[Seed 99] Epoch [55/400] Train Loss: 0.000627 | Val Loss: 0.200986\n[Seed 99] Epoch [56/400] Train Loss: 0.000634 | Val Loss: 0.200644\n[Seed 99] Epoch [57/400] Train Loss: 0.000633 | Val Loss: 0.201310\n[Seed 99] Early stopping at epoch 57\n\n=== Training Model (Seed 2025) ===\n[Seed 2025] Epoch [1/400] Train Loss: 0.002511 | Val Loss: 0.247046\n[Seed 2025] Epoch [2/400] Train Loss: 0.001904 | Val Loss: 0.229871\n[Seed 2025] Epoch [3/400] Train Loss: 0.001740 | Val Loss: 0.220533\n[Seed 2025] Epoch [4/400] Train Loss: 0.001622 | Val Loss: 0.212984\n[Seed 2025] Epoch [5/400] Train Loss: 0.001527 | Val Loss: 0.207683\n[Seed 2025] Epoch [6/400] Train Loss: 0.001432 | Val Loss: 0.203627\n[Seed 2025] Epoch [7/400] Train Loss: 0.001367 | Val Loss: 0.201979\n[Seed 2025] Epoch [8/400] Train Loss: 0.001311 | Val Loss: 0.199898\n[Seed 2025] Epoch [9/400] Train Loss: 0.001250 | Val Loss: 0.199251\n[Seed 2025] Epoch [10/400] Train Loss: 0.001200 | Val Loss: 0.199555\n[Seed 2025] Epoch [11/400] Train Loss: 0.001154 | Val Loss: 0.199011\n[Seed 2025] Epoch [12/400] Train Loss: 0.001114 | Val Loss: 0.199387\n[Seed 2025] Epoch [13/400] Train Loss: 0.001074 | Val Loss: 0.199339\n[Seed 2025] Epoch [14/400] Train Loss: 0.001049 | Val Loss: 0.198238\n[Seed 2025] Epoch [15/400] Train Loss: 0.001018 | Val Loss: 0.196611\n[Seed 2025] Epoch [16/400] Train Loss: 0.000982 | Val Loss: 0.195804\n[Seed 2025] Epoch [17/400] Train Loss: 0.000955 | Val Loss: 0.195711\n[Seed 2025] Epoch [18/400] Train Loss: 0.000935 | Val Loss: 0.197618\n[Seed 2025] Epoch [19/400] Train Loss: 0.000913 | Val Loss: 0.197238\n[Seed 2025] Epoch [20/400] Train Loss: 0.000885 | Val Loss: 0.195774\n[Seed 2025] Epoch [21/400] Train Loss: 0.000866 | Val Loss: 0.197215\n[Seed 2025] Epoch [22/400] Train Loss: 0.000852 | Val Loss: 0.198481\n[Seed 2025] Epoch [23/400] Train Loss: 0.000830 | Val Loss: 0.197521\n[Seed 2025] Epoch [24/400] Train Loss: 0.000812 | Val Loss: 0.198659\n[Seed 2025] Epoch [25/400] Train Loss: 0.000799 | Val Loss: 0.197341\n[Seed 2025] Epoch [26/400] Train Loss: 0.000780 | Val Loss: 0.198062\n[Seed 2025] Epoch [27/400] Train Loss: 0.000759 | Val Loss: 0.199419\n[Seed 2025] Epoch [28/400] Train Loss: 0.000755 | Val Loss: 0.200088\n[Seed 2025] Epoch [29/400] Train Loss: 0.000738 | Val Loss: 0.200720\n[Seed 2025] Epoch [30/400] Train Loss: 0.000730 | Val Loss: 0.200317\n[Seed 2025] Epoch [31/400] Train Loss: 0.000719 | Val Loss: 0.199928\n[Seed 2025] Epoch [32/400] Train Loss: 0.000701 | Val Loss: 0.199069\n[Seed 2025] Epoch [33/400] Train Loss: 0.000693 | Val Loss: 0.199675\n[Seed 2025] Epoch [34/400] Train Loss: 0.000689 | Val Loss: 0.200034\n[Seed 2025] Epoch [35/400] Train Loss: 0.000680 | Val Loss: 0.198787\n[Seed 2025] Epoch [36/400] Train Loss: 0.000672 | Val Loss: 0.199082\n[Seed 2025] Epoch [37/400] Train Loss: 0.000672 | Val Loss: 0.199553\n[Seed 2025] Epoch [38/400] Train Loss: 0.000659 | Val Loss: 0.199826\n[Seed 2025] Epoch [39/400] Train Loss: 0.000656 | Val Loss: 0.200244\n[Seed 2025] Epoch [40/400] Train Loss: 0.000649 | Val Loss: 0.200135\n[Seed 2025] Epoch [41/400] Train Loss: 0.000647 | Val Loss: 0.199224\n[Seed 2025] Epoch [42/400] Train Loss: 0.000641 | Val Loss: 0.199595\n[Seed 2025] Epoch [43/400] Train Loss: 0.000645 | Val Loss: 0.199158\n[Seed 2025] Epoch [44/400] Train Loss: 0.000638 | Val Loss: 0.199092\n[Seed 2025] Epoch [45/400] Train Loss: 0.000633 | Val Loss: 0.199218\n[Seed 2025] Epoch [46/400] Train Loss: 0.000629 | Val Loss: 0.199347\n[Seed 2025] Epoch [47/400] Train Loss: 0.000628 | Val Loss: 0.199422\n[Seed 2025] Epoch [48/400] Train Loss: 0.000631 | Val Loss: 0.199380\n[Seed 2025] Epoch [49/400] Train Loss: 0.000628 | Val Loss: 0.199430\n[Seed 2025] Epoch [50/400] Train Loss: 0.000632 | Val Loss: 0.199404\n[Seed 2025] Epoch [51/400] Train Loss: 0.000629 | Val Loss: 0.199404\n[Seed 2025] Epoch [52/400] Train Loss: 0.000627 | Val Loss: 0.199385\n[Seed 2025] Epoch [53/400] Train Loss: 0.000628 | Val Loss: 0.199337\n[Seed 2025] Epoch [54/400] Train Loss: 0.000632 | Val Loss: 0.199319\n[Seed 2025] Epoch [55/400] Train Loss: 0.000634 | Val Loss: 0.199336\n[Seed 2025] Epoch [56/400] Train Loss: 0.000632 | Val Loss: 0.199320\n[Seed 2025] Epoch [57/400] Train Loss: 0.000630 | Val Loss: 0.199150\n[Seed 2025] Early stopping at epoch 57\n\n=====================\nFinal Ensemble R²: 0.6268\nFinal Ensemble MAE: 6.2809\n=====================\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport torch\n\n# ======================================================\n# STEP 1 — Load Full Test Embeddings\n# ======================================================\ntest_embeddings = np.load(\"/kaggle/input/embeddings/full_embeddings.npy\")\nprint(\"Loaded full test embeddings:\", test_embeddings.shape)\n\n# ======================================================\n# STEP 2 — Apply SAME SCALER & PCA (from training)\n# ======================================================\n# ⚠️ Use the same scaler and PCA objects from training\n# (Do NOT re-fit them — just transform)\n\nX_test_scaled = scaler.transform(test_embeddings)\nX_test_reduced = pca.transform(X_test_scaled)\nprint(\"Reduced test shape:\", X_test_reduced.shape)\n\n# Convert to tensor\nX_test_tensor = torch.tensor(X_test_reduced, dtype=torch.float32).to(device)\n\n# ======================================================\n# STEP 3 — Ensemble Predictions\n# ======================================================\nensemble_preds = []\n\nfor idx, model in enumerate(models):\n    model.eval()\n    with torch.no_grad():\n        preds_log = model(X_test_tensor).cpu().numpy()\n        preds = np.expm1(preds_log)  # inverse of log1p\n        ensemble_preds.append(preds)\n        print(f\"Model {idx+1} done.\")\n\n# Average predictions\nfinal_preds = np.mean(ensemble_preds, axis=0)\nfinal_preds = np.maximum(final_preds, 0)  # avoid negatives\n\nprint(\"\\n✅ Final prediction shape:\", final_preds.shape)\n\n# # ======================================================\n# # STEP 4 — Save Output\n# # ======================================================\n# np.save(\"submission.npy\", final_preds)\n# print(\"Saved → submission.npy\")\n\n# # (Optional) Save as CSV if required\n# import pandas as pd\n# pd.DataFrame({\"price\": final_preds.flatten()}).to_csv(\"submission.csv\", index=False)\n# print(\"Saved → submission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:27:58.237964Z","iopub.execute_input":"2025-10-13T17:27:58.238708Z","iopub.status.idle":"2025-10-13T17:28:03.911051Z","shell.execute_reply.started":"2025-10-13T17:27:58.238682Z","shell.execute_reply":"2025-10-13T17:28:03.910179Z"}},"outputs":[{"name":"stdout","text":"Loaded full test embeddings: (75000, 2048)\nReduced test shape: (75000, 512)\nModel 1 done.\nModel 2 done.\nModel 3 done.\n\n✅ Final prediction shape: (75000, 1)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# (Optional) Save as CSV if required\nimport pandas as pd\npd.DataFrame({\"price\": final_preds.flatten()}).to_csv(\"/kaggle/working/test_out.csv\", index=False)\nprint(\"Saved → submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:29:02.884290Z","iopub.execute_input":"2025-10-13T17:29:02.884844Z","iopub.status.idle":"2025-10-13T17:29:02.975332Z","shell.execute_reply.started":"2025-10-13T17:29:02.884821Z","shell.execute_reply":"2025-10-13T17:29:02.974678Z"}},"outputs":[{"name":"stdout","text":"Saved → submission.csv\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}